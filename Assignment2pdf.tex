\documentclass{article}
\usepackage{minted}
\usemintedstyle{colorful}
\usepackage{changepage}
\usepackage{geometry}[margins = 1cm]
%\usepackage{booktabs}
\title{Assignment 2}
\begin{document}
\maketitle

\definecolor{bg}{rgb}{0.95, 0.95, 0.95}
This document describes our process to code the conditional and unconditional likelihood functions for the AR(7) process based on the starter code provided in the assignment.
Furthermore, it outlines our approach to estimate the AR(7) paramteres for the \texttt{INDPRO} variable from the FRED-MD dataset and our forecast.

\tableofcontents

\section{Conditional Likelihood}
\subsection{Theoretical Outline}
The AR(7) Model is defined as:
\begin{equation}
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \phi_3 y_{t-3} + \phi_4 y_{t-4} + \phi_5 y_{t-5} + \phi_6 y_{t-6} + \phi_7 y_{t-7} + u_t
\end{equation}
where $u_t \sim N(0, \sigma^2)$.

We can then define the conditional likelihood function as:
\begin{equation}
L(y,\phi) = f(y_1, y_2, y_3, y_4, y_5, y_6, y_7;\phi)\prod_{t=8}^Tf(y_t|y_{t-1},..., y_{t-7};\phi)
\end{equation}
The conditional log-likelihood function is:
\begin{equation}
\ell (y; \phi) = \log( f(y_1,..., y_7;\phi))+\sum_{t=8}^T \log (f(y_t|y_{t-1},..., y_{t-7};\phi))
\end{equation}

We regard the first 7 observation or their joint distribution as a determenistic quantity and thus:

\begin{equation}
\ell (y; \phi) \propto \sum_{t=8}^T \log(f(y_t|y_{t-1},..., y_{t-7};\phi))
\end{equation}
Also, since the error terms are normally distributed with mean 0 and varieance $\sigma^2$, we can determine the conditional distribution of $y_t$ as:
\begin{equation}
y_t|y_{t-1},y_{t-2}, ..., y_{t-7} \sim N(c + \phi_1y_{t-1} + ... + \phi_7 y_{t-7}, \sigma^2)
\end{equation}
Therefore, to implement the conditional log-likelihood function we can code it as the sum of normally distributed variables.

\begin{equation}
\ell_C (y; \phi) \propto \sum_{t=8}^T \log \left( \frac{1}{\sigma \sqrt{2\pi}} \exp \left\{-\frac{(y_t - (c+ \phi_1y_{t-1} + ... + \phi_7 y_{t-7} ))^2}{2\sigma^2} \right\} \right)
\end{equation}
\subsection{Implementation in Python}

\begin{minted}[bgcolor=bg]{python}
from scipy.stats import norm
from scipy.stats import multivariate_normal
import numpy as np

def lagged_matrix(Y, max_lag=7):
    n = len(Y)
    lagged_matrix = np.full((n, max_lag), np.nan)    
    # Fill each column with the appropriately lagged data
    for lag in range(1, max_lag + 1):
        lagged_matrix[lag:, lag - 1] = Y[:-lag]
    return lagged_matrix


def cond_loglikelihood_ar7(params, y):
    c = params[0] 
    phi = params[1:8]
    sigma2 = params[8]
    mu, Sigma, stationary = unconditional_ar_mean_variance(c, phi, sigma2)
    ## We could check that at phis the process is stationary and return -Inf if it is not
    if not(stationary):
        return -np.inf
    ## The distribution of 
    # y_t|y_{t-1}, ..., y_{t-7} ~ N(c+\phi_{1}*y_{t-1}+...+\phi_{7}y_{t-7}, sigma2)
    ## Create lagged matrix
    X = lagged_matrix(y, 7)
    yf = y[7:]
    Xf = X[7:,:]
    loglik = np.sum(norm.logpdf(yf, loc=(c + Xf@phi), scale=np.sqrt(sigma2)))
    return loglik
\end{minted}

\section{Unconditional Likelihood}
\subsection{Theoretical Outline}
The unconditional likelihood function starts from a similar approach.
However, instead of regarding the first 7 observations as determenistic it regards them as multivariate normally distributed with mean $\mu$ and covariance $\Sigma$.
\begin{equation}
\ell_U (y; \phi) = \log( f(y_1,..., y_7;\phi, \mu, \Sigma))+\sum_{t=8}^T \log (f(y_t|y_{t-1},..., y_{t-7};\phi))
\end{equation}
The second term on the right hand side is the conditional log-likelihood, the first term is the multivariate normal distribution of the first seven observations.

As described in the assignment the parameters of the multivariate normal distribution can be obtained through:
\begin{equation}
\mu = (I - A)^{-1}b
\end{equation}
to compute the mean and the Lyapunov equation
\begin{equation}
\Sigma = A \Sigma A^T + Q
\end{equation}
to compute the covariance matrix of the autoregressive process.

\subsection{Implementation in Python}
First, the matrix $A$ is constructed, which is a square matrix where the first row is made up of $\phi_1, \phi_2, ..., \phi_p$ of the autoregressive process of order $p$ and the other rows are in reduced row echelon form.

For this the command \mintinline{python}{np.zeros((p,p))} is used, which creates an array of dimension $p \times p$ filled with zeros. 
Next, we replace the first row with $\phi_1, \phi_2, ..., \phi_p$ by subsetting $A$ with the command \mintinline{python}{A[0, :] = phis}.
Next, to fill the remaining rows except the last row with leading ones the comman np.eye is used.

Thus for given $p$ and vector $\phi$, this code creates the matrix $A$ and then calculates the mean and the covariance. 
The code also checks whether the AR(7) process is stationary with respect to the given parameters.
\begin{adjustwidth}{-2cm}{-2cm}
\begin{minted}[bgcolor=bg]{python}
def unconditional_ar_mean_variance(c, phis, sigma2):
    ## The length of phis is p
    p = len(phis)
    A = np.zeros((p, p))
    A[0, :] = phis
    A[1:, 0:(p-1)] = np.eye(p-1)
    ## Check for stationarity
    eigA = np.linalg.eig(A)
    if all(np.abs(eigA.eigenvalues)<1):
        stationary = True
    else:
        stationary = False
    # Create the vector b
    b = np.zeros((p, 1))
    b[0, 0] = c
    
    # Compute the mean using matrix algebra
    I = np.eye(p)
    mu = np.linalg.inv(I - A) @ b
    
    # Solve the discrete Lyapunov equation
    Q = np.zeros((p, p))
    Q[0, 0] = sigma2
    #Sigma = np.linalg.solve(I - np.kron(A, A), Q.flatten()).reshape(7, 7)
    Sigma = scipy.linalg.solve_discrete_lyapunov(A, Q)
    
    return mu.ravel(), Sigma, stationary
\end{minted}
\end{adjustwidth}

This function is used to build the unconditional likelihood function:

\begin{minted}[bgcolor=bg]{python}
def uncond_loglikelihood_ar7(params, y):
    ## The unconditional loglikelihood
    ## is the unconditional "plus" the density of the
    ## first p (7 in our case) observations
    cloglik = cond_loglikelihood_ar7(params, y)

    ## Calculate initial
    # y_1, ..., y_7 ~ N(mu, sigma_y)
    c = params[0] 
    phi = params[1:8]
    sigma2 = params[8]
    mu, Sigma, stationary = unconditional_ar_mean_variance(c, phi, sigma2)
    if not(stationary):
        return -np.inf
    mvn = multivariate_normal(mean=mu, cov=Sigma, allow_singular=True)
    uloglik = cloglik + mvn.logpdf(y[0:7])
    return uloglik
\end{minted}
\section{Maximum Likelihood Estimation for \texttt{INDPRO}}
As in assignment 1, we import the FRED-MD dataset using \texttt{pandas}.
Since we are only interested in the \texttt{INDPRO} variable, we select it and transform it using log differences:
\begin{minted}[bgcolor=bg]{python}
import pandas as pd
import numpy as np
#Read Data
df = df = pd.read_csv('~/Downloads/current.csv')
#Select INDPRO
INDPRO = df['INDPRO']
#Drop first Row
INDPRO = INDPRO.drop(index=0)
#transform INDPRO using log differences
INDPRO = np.log(INDPRO).diff()

#implement Starter Code from the assignment
## Lagged Matrix Function
def lagged_matrix(Y, max_lag=7):
    n = len(Y)
    lagged_matrix = np.full((n, max_lag), np.nan)    
    # Fill each column with the appropriately lagged data
    for lag in range(1, max_lag + 1):
        lagged_matrix[lag:, lag - 1] = Y[:-lag]
    return lagged_matrix

## Mean- Variance - Stationarity Function
def unconditional_ar_mean_variance(c, phis, sigma2):
    ## The length of phis is p
    p = len(phis)
    A = np.zeros((p, p))
    A[0, :] = phis
    A[1:, 0:(p-1)] = np.eye(p-1)
    ## Check for stationarity
    eigA = np.linalg.eig(A)
    if all(np.abs(eigA.eigenvalues)<1):
        stationary = True
    else:
        stationary = False
    # Create the vector b
    b = np.zeros((p, 1))
    b[0, 0] = c
    
    # Compute the mean using matrix algebra
    I = np.eye(p)
    mu = np.linalg.inv(I - A) @ b
    
    # Solve the discrete Lyapunov equation
    Q = np.zeros((p, p))
    Q[0, 0] = sigma2
    #Sigma = np.linalg.solve(I - np.kron(A, A), Q.flatten()).reshape(7, 7)
    Sigma = scipy.linalg.solve_discrete_lyapunov(A, Q)
    
    return mu.ravel(), Sigma, stationary

## Conditional Likelihood
def cond_loglikelihood_ar7(params, y):
    c = params[0] 
    phi = params[1:8]
    sigma2 = params[8]
    mu, Sigma, stationary = unconditional_ar_mean_variance(c, phi, sigma2)
    ## We could check that at phis the process is stationary and return -Inf if it is not
    if not(stationary):
        return -np.inf
    ## The distribution of 
    # y_t|y_{t-1}, ..., y_{t-7} ~ N(c+\phi_{1}*y_{t-1}+...+\phi_{7}y_{t-7}, sigma2)
    ## Create lagged matrix
    X = lagged_matrix(y, 7)
    yf = y[7:]
    Xf = X[7:,:]
    loglik = np.sum(norm.logpdf(yf, loc=(c + Xf@phi), scale=np.sqrt(sigma2)))
    return loglik

## Unconditional Likelihood
def uncond_loglikelihood_ar7(params, y):
    ## The unconditional loglikelihood
    ## is the unconditional "plus" the density of the
    ## first p (7 in our case) observations
    cloglik = cond_loglikelihood_ar7(params, y)

    ## Calculate initial
    # y_1, ..., y_7 ~ N(mu, sigma_y)
    c = params[0] 
    phi = params[1:8]
    sigma2 = params[8]
    mu, Sigma, stationary = unconditional_ar_mean_variance(c, phi, sigma2)
    if not(stationary):
        return -np.inf
    mvn = multivariate_normal(mean=mu, cov=Sigma, allow_singular=True)
    uloglik = cloglik + mvn.logpdf(y[0:7])
    return uloglik
    
# Using INDPRO as the target variable.
## Computing OLS
X = lagged_matrix(INDPRO, 7)
yf = INDPRO[7:]
Xf = np.hstack((np.ones((len(INDPRO)-7,1)), X[7:,:]))
beta = np.linalg.solve(Xf.T@Xf, Xf.T@yf)
sigma2_hat = np.mean((yf - Xf@beta)**2)
params= np.hstack((beta, sigma2_hat))
print("The parameters of the OLS model are", params)

# to maximize likelihood a function of the negative likelihood is defined to be minimized
params = np.array([
    0.0012, ## c
    0.0291, 0.07, 0.059, 0.04, 0.04, 0.02, 0.06, ## phi
    0.008 ## sigma2    
    ])

def cobj(params, y): 
    return - cond_loglikelihood_ar7(params,y)

# Maximizing the likelihood
results = scipy.optimize.minimize(fun = cobj, x0 =  params, args = INDPRO, method='L-BFGS-B')
print("The parameters estimated by maximizing the conditional likelihood are:", results.x)

#Same Procedure for unconditional likelihood
params= np.hstack((beta, sigma2_hat))

def uobj(params, y): 
    return - uncond_loglikelihood_ar7(params,y)

results = scipy.optimize.minimize(uobj, params, args = INDPRO, method='Nelder-Mead')
print("The parameters estimated by maximizing the unconditional likelihood are:", results.x)

\end{minted}
As the next step we construct the matrix containing \texttt{INDPRO} and its lagged values .

\subsection{Results}
We estimate four different models for the conditional likelihood:
\begin{enumerate}
\item \texttt{Model 1}: We use the OLS coefficients as the initial guess and minimize the conditional likelihood function using the 'L-BFGS-B' method.
\item \texttt{Model 2}: We use the OLS coefficients as the initial guess and minimize the conditional likelihood function using the 'Neler-Mead' method.
\item \texttt{Model 3}: We supply a different initial guess and minimize the conditional likelihood function using the 'L-BFGS-B' method.
\item \texttt{Model 4}: We supply a different initial guess and minimize the conditional likelihood function using the  'Neler-Mead' method.
\end{enumerate}

\begin{minted}[bgcolor=bg]{python}

# Now we estimate multiple models:
## OLS model
modOLS = np.hstack((beta, sigma2_hat))
## Conditional Likelihood
## 1. Using the OLS parameters as the initial guess
### L-BFGS-B
mod1 = scipy.optimize.minimize(fun = cobj, x0 =  modOLS, args = INDPRO, method='L-BFGS-B').x
### Nelder-Mead
mod2 = scipy.optimize.minimize(fun = cobj, x0 =  modOLS, args = INDPRO, method='Nelder-Mead').x
## 2. Using a slightly different initial guess
Initial_Guess = np.array([
    0.0012, ## c
    0.0291, 0.07, 0.059, 0.04, 0.04, 0.02, 0.06, ## phi
    0.009 ## sigma2 
])
### L-BFGS-B
mod3 = scipy.optimize.minimize(fun = cobj, x0 =  Initial_Guess, args = INDPRO, method='L-BFGS-B').x
### Nelder-Mead
mod4 = scipy.optimize.minimize(fun = cobj, x0 =  Initial_Guess, args = INDPRO, method='Nelder-Mead').x

## Unbounded Unconditional Likelihood
## 1. Using the OLS parameters as the initial guess
### L-BFGS-B
mod5 = scipy.optimize.minimize(fun = uobj, x0 =  modOLS, args = INDPRO, method='L-BFGS-B').x
### Nelder-Mead
mod6 = scipy.optimize.minimize(fun = uobj, x0 =  modOLS, args = INDPRO, method='Nelder-Mead').x
## 2. Using a slightly different initial guess
Initial_Guess = np.array([
    0.0012, ## c
    0.00291, 0.007, 0.0509, 0.0024, 0.0409, 0.012, 0.0601, ## phi
    0.009 ## sigma2 
])
### L-BFGS-B
#mod7 = scipy.optimize.minimize(fun = uobj, x0 =  Initial_Guess, args = INDPRO, method='L-BFGS-B').x
### Nelder-Mead
#mod8 = scipy.optimize.minimize(fun = uobj, x0 =  Initial_Guess, args = INDPRO, method='Nelder-Mead').x

mods = np.array([modOLS, mod1, mod2, mod3, mod4])
print(mods)
#np.savetxt("mods.txt", mods)
modsDF = pd.DataFrame(mods)
modsDF = modsDF.T
print(modsDF)
print(modsDF.to_latex())
\end{minted}

The results are displayed in Table 1.
The results show that if the initial guess are the OLS coefficients, the conditional MLE correctly finds identical coefficients. However, when we supply a different initial guess, the minimization procedure using 'L-BFGS-B' returns simply the same initial guess while using the 'Neler-Mead' method changes the estimated coefficients quite dramatically.
This could hint towards errors in our code, multiple local minima or a non-continuous likelihood function.

\begin{table}[ht]

\begin{tabular}{lrrrrrrr|}
\hline
\multicolumn{1}{|c}{Coefficients} & OLS & \multicolumn{1}{c}{Model 1} & \multicolumn{1}{c}{Model 2} & \multicolumn{1}{c}{Model 3} & \multicolumn{1}{l|}{Model 4} \\ \hline
\multicolumn{1}{|c|}{c}    & 0.001263 & 0.001263 & 0.001263 & 0.001200 & 0.000874 \\
\multicolumn{1}{|c|}{$\phi_1$} & 0.291263 & 0.291263 & 0.291263 & 0.029100 & 0.056374 \\
\multicolumn{1}{|c|}{$\phi_2$} & -0.074517 & -0.074517 & -0.074517 & 0.070000 & 0.017666 \\
\multicolumn{1}{|c|}{$\phi_3$} & 0.046809 & 0.046809 & 0.046809 & 0.059000 & 0.060468 \\
\multicolumn{1}{|c|}{$\phi_4$} & 0.047565 & 0.047565 & 0.047565 & 0.040000 & 0.009790 \\
\multicolumn{1}{|c|}{$\phi_5$}& -0.024910 & -0.024910 & -0.024910 & 0.040000 & 0.058602 \\
\multicolumn{1}{|c|}{$\phi_6$} & 0.061501 & 0.061501 & 0.061501 & 0.020000 & 0.038055 \\
\multicolumn{1}{|c|}{$\phi_7$} & 0.020106 & 0.020106 & 0.020106 & 0.060000 & 0.114138 \\
\multicolumn{1}{|c|}{$\sigma^2$} & 0.000089 & 0.000089 & 0.000089 & 0.009000 & 0.000095 \\
\hline
\end{tabular}
\caption{Estimation results for MLE using the conditional likelihood function. Colums OLS reports the OLS coefficients.}
\end{table}

\section{Forecast}
\end{document}